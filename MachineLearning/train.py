#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº”å­æ£‹AIè®­ç»ƒæ¡†æ¶
å®ç°è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå’Œæ¨¡å‹ä¼˜åŒ–
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import os
import json
import time
from datetime import datetime
from collections import deque
import random
from copy import deepcopy

from board import GomokuBoard
from mcts import MCTS
from net import GomokuNet, GomokuTrainer, GomokuDataset
from game import GomokuAI, GameEngine, MCTSWithNet

class SelfPlayDataset(Dataset):
    """è‡ªæˆ‘å¯¹å¼ˆæ•°æ®é›†"""
    
    def __init__(self, max_size=10000):
        self.data = deque(maxlen=max_size)
        self.max_size = max_size
    
    def add_game_data(self, game_states, game_policies, game_values):
        """æ·»åŠ ä¸€å±€æ¸¸æˆçš„æ•°æ®"""
        for state, policy, value in zip(game_states, game_policies, game_values):
            self.data.append((state, policy, value))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        state, policy, value = self.data[idx]
        return {
            'board_state': torch.FloatTensor(state),
            'policy': torch.FloatTensor(policy),
            'value': torch.FloatTensor([value])
        }
    
    def clear(self):
        """æ¸…ç©ºæ•°æ®"""
        self.data.clear()
    
    def save_to_file(self, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        data_list = list(self.data)
        torch.save(data_list, filename)
        print(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
    
    def load_from_file(self, filename):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
        if os.path.exists(filename):
            data_list = torch.load(filename)
            self.data.extend(data_list)
            print(f"ä» {filename} åŠ è½½äº† {len(data_list)} æ¡è®­ç»ƒæ•°æ®")
        else:
            print(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")

class MCTSTrainer:
    """MCTSè®­ç»ƒå™¨ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®"""
    
    def __init__(self, net, board_size=9, mcts_simulations=100, temperature=1.0):
        self.net = net
        self.board_size = board_size
        self.mcts_simulations = mcts_simulations
        self.temperature = temperature
        
    def generate_move_probabilities(self, board_state, mcts_tree=None):
        """ä½¿ç”¨MCTSç”Ÿæˆç§»åŠ¨æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            board_state: å½“å‰æ£‹ç›˜çŠ¶æ€
            mcts_tree: å¯é€‰çš„MCTSæ ‘ï¼ˆç”¨äºå¤ç”¨ï¼‰
            
        Returns:
            move_probs: ç§»åŠ¨æ¦‚ç‡åˆ†å¸ƒ
            mcts_tree: æ›´æ–°åçš„MCTSæ ‘
        """
        # åˆ›å»ºMCTSå®ä¾‹
        if mcts_tree is None:
            mcts = MCTSWithNet(
                net=self.net,
                time_limit=None,  # ä½¿ç”¨è¿­ä»£æ¬¡æ•°é™åˆ¶
                max_iterations=self.mcts_simulations
            )
        else:
            mcts = mcts_tree
        
        # è¿›è¡ŒMCTSæœç´¢
        root = mcts._create_root_node(board_state)
        
        for _ in range(self.mcts_simulations):
            # é€‰æ‹©
            node = mcts._select(root)
            
            # æ‰©å±•
            if not node.is_terminal():
                node = mcts._expand(node)
            
            # æ¨¡æ‹Ÿ
            value = mcts._simulate(node)
            
            # åå‘ä¼ æ’­
            mcts._backpropagate(node, value)
        
        # è®¡ç®—ç§»åŠ¨æ¦‚ç‡
        move_probs = np.zeros(board_state.size * board_state.size)
        
        if root.children:
            visits = np.array([child.visits for child in root.children.values()])
            
            if self.temperature == 0:
                # è´ªå©ªé€‰æ‹©
                best_moves = []
                max_visits = np.max(visits)
                for move, child in root.children.items():
                    if child.visits == max_visits:
                        best_moves.append(move)
                
                # åœ¨æœ€ä½³ç§»åŠ¨ä¸­å‡åŒ€åˆ†å¸ƒ
                for move in best_moves:
                    move_idx = move[0] * board_state.size + move[1]
                    move_probs[move_idx] = 1.0 / len(best_moves)
            else:
                # æ¸©åº¦é‡‡æ ·
                if self.temperature != 1.0:
                    visits = visits ** (1.0 / self.temperature)
                
                visits_sum = np.sum(visits)
                if visits_sum > 0:
                    for i, (move, child) in enumerate(root.children.items()):
                        move_idx = move[0] * board_state.size + move[1]
                        move_probs[move_idx] = visits[i] / visits_sum
        
        return move_probs, mcts
    
    def play_self_game(self, verbose=False):
        """è¿›è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆ
        
        Returns:
            game_states: æ¸¸æˆçŠ¶æ€åˆ—è¡¨
            game_policies: ç­–ç•¥æ¦‚ç‡åˆ—è¡¨
            game_values: ä»·å€¼è¯„ä¼°åˆ—è¡¨
        """
        board = GomokuBoard(size=self.board_size)
        game_states = []
        game_policies = []
        
        mcts_tree = None
        move_count = 0
        
        if verbose:
            print("å¼€å§‹è‡ªæˆ‘å¯¹å¼ˆ...")
            board.display()
        
        while True:
            # æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
            game_over, winner = board.is_game_over()
            if game_over:
                break
            
            # è®°å½•å½“å‰çŠ¶æ€
            current_state = self._board_to_input(board)
            game_states.append(current_state)
            
            # ç”Ÿæˆç§»åŠ¨æ¦‚ç‡
            move_probs, mcts_tree = self.generate_move_probabilities(board, mcts_tree)
            game_policies.append(move_probs)
            
            # æ ¹æ®æ¦‚ç‡é€‰æ‹©ç§»åŠ¨
            valid_moves = board.get_valid_moves()
            if not valid_moves:
                break
            
            # è®¡ç®—æœ‰æ•ˆç§»åŠ¨çš„æ¦‚ç‡
            valid_probs = []
            for move in valid_moves:
                move_idx = move[0] * board.size + move[1]
                valid_probs.append(move_probs[move_idx])
            
            valid_probs = np.array(valid_probs)
            if np.sum(valid_probs) > 0:
                valid_probs = valid_probs / np.sum(valid_probs)
                selected_idx = np.random.choice(len(valid_moves), p=valid_probs)
            else:
                selected_idx = np.random.choice(len(valid_moves))
            
            selected_move = valid_moves[selected_idx]
            
            # æ‰§è¡Œç§»åŠ¨
            board.make_move(selected_move[0], selected_move[1])
            move_count += 1
            
            if verbose and move_count <= 10:  # åªæ˜¾ç¤ºå‰10æ­¥
                print(f"ç§»åŠ¨ {move_count}: {selected_move}")
                board.display()
            
            # é‡ç½®MCTSæ ‘ï¼ˆç®€åŒ–å®ç°ï¼‰
            mcts_tree = None
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼ˆä»æ¸¸æˆç»“æœåæ¨ï¼‰
        game_values = []
        for i, state in enumerate(game_states):
            # å½“å‰ç©å®¶è§†è§’çš„ä»·å€¼
            current_player = 1 if i % 2 == 0 else -1
            if winner == current_player:
                value = 1.0
            elif winner == -current_player:
                value = -1.0
            else:
                value = 0.0
            game_values.append(value)
        
        if verbose:
            result_str = "é»‘èƒœ" if winner == 1 else "ç™½èƒœ" if winner == -1 else "å¹³å±€"
            print(f"è‡ªæˆ‘å¯¹å¼ˆç»“æŸ: {result_str}, æ€»æ­¥æ•°: {move_count}")
        
        return game_states, game_policies, game_values
    
    def _board_to_input(self, board):
        """å°†æ£‹ç›˜è½¬æ¢ä¸ºç¥ç»ç½‘ç»œè¾“å…¥æ ¼å¼"""
        # åˆ›å»º3é€šé“è¾“å…¥ï¼šå½“å‰ç©å®¶æ£‹å­ã€å¯¹æ‰‹æ£‹å­ã€å½“å‰ç©å®¶æ ‡è¯†
        input_planes = np.zeros((3, board.size, board.size))
        
        # å½“å‰ç©å®¶çš„æ£‹å­
        input_planes[0] = (board.board == board.current_player).astype(np.float32)
        
        # å¯¹æ‰‹çš„æ£‹å­
        input_planes[1] = (board.board == -board.current_player).astype(np.float32)
        
        # å½“å‰ç©å®¶æ ‡è¯†ï¼ˆå…¨1è¡¨ç¤ºå½“å‰ç©å®¶å›åˆï¼‰
        input_planes[2] = np.ones((board.size, board.size)) * (board.current_player == 1)
        
        return input_planes

class AlphaZeroTrainer:
    """AlphaZeroé£æ ¼çš„è®­ç»ƒå™¨"""
    
    def __init__(self, board_size=9, num_channels=64, learning_rate=0.001):
        self.board_size = board_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºç¥ç»ç½‘ç»œ
        self.net = GomokuNet(board_size=board_size, num_channels=num_channels)
        self.net.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate, weight_decay=1e-4)
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset = SelfPlayDataset(max_size=50000)
        
        # åˆ›å»ºMCTSè®­ç»ƒå™¨
        self.mcts_trainer = MCTSTrainer(
            net=self.net,
            board_size=board_size,
            mcts_simulations=100,
            temperature=1.0
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'iterations': 0,
            'self_play_games': 0,
            'training_losses': [],
            'policy_losses': [],
            'value_losses': []
        }
    
    def self_play_iteration(self, num_games=10, verbose=False):
        """è¿›è¡Œä¸€è½®è‡ªæˆ‘å¯¹å¼ˆ"""
        print(f"å¼€å§‹è‡ªæˆ‘å¯¹å¼ˆ: {num_games} å±€æ¸¸æˆ")
        
        self.net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        games_data = []
        for game_num in range(num_games):
            if verbose or (game_num + 1) % max(1, num_games // 5) == 0:
                print(f"è‡ªæˆ‘å¯¹å¼ˆè¿›åº¦: {game_num + 1}/{num_games}")
            
            # è¿›è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆ
            states, policies, values = self.mcts_trainer.play_self_game(verbose=False)
            
            if states:  # ç¡®ä¿æ¸¸æˆæœ‰æ•ˆ
                games_data.append((states, policies, values))
                self.dataset.add_game_data(states, policies, values)
        
        self.training_stats['self_play_games'] += len(games_data)
        print(f"è‡ªæˆ‘å¯¹å¼ˆå®Œæˆ: ç”Ÿæˆ {len(games_data)} å±€æœ‰æ•ˆæ¸¸æˆ")
        print(f"æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        
        return len(games_data)
    
    def train_network(self, epochs=10, batch_size=32, verbose=True):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        if len(self.dataset) < batch_size:
            print(f"æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {batch_size} æ¡æ•°æ®ï¼Œå½“å‰åªæœ‰ {len(self.dataset)} æ¡")
            return
        
        print(f"å¼€å§‹è®­ç»ƒç½‘ç»œ: {epochs} è½®, æ‰¹å¤§å°: {batch_size}")
        
        self.net.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        
        epoch_losses = []
        epoch_policy_losses = []
        epoch_value_losses = []
        
        for epoch in range(epochs):
            total_loss = 0.0
            total_policy_loss = 0.0
            total_value_loss = 0.0
            num_batches = 0
            
            for batch in dataloader:
                board_states = batch['board_state'].to(self.device)
                target_policies = batch['policy'].to(self.device)
                target_values = batch['value'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                pred_policies, pred_values = self.net(board_states)
                
                # è®¡ç®—æŸå¤±
                policy_loss = nn.CrossEntropyLoss()(pred_policies, target_policies)
                value_loss = nn.MSELoss()(pred_values.squeeze(), target_values.squeeze())
                total_loss_batch = policy_loss + value_loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss_batch.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += total_loss_batch.item()
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                num_batches += 1
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            avg_policy_loss = total_policy_loss / num_batches
            avg_value_loss = total_value_loss / num_batches
            
            epoch_losses.append(avg_loss)
            epoch_policy_losses.append(avg_policy_loss)
            epoch_value_losses.append(avg_value_loss)
            
            if verbose:
                print(f"Epoch {epoch + 1}/{epochs}: "
                      f"Loss={avg_loss:.4f}, "
                      f"Policy={avg_policy_loss:.4f}, "
                      f"Value={avg_value_loss:.4f}")
        
        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['training_losses'].extend(epoch_losses)
        self.training_stats['policy_losses'].extend(epoch_policy_losses)
        self.training_stats['value_losses'].extend(epoch_value_losses)
        
        print(f"ç½‘ç»œè®­ç»ƒå®Œæˆ")
        return epoch_losses
    
    def training_iteration(self, self_play_games=10, training_epochs=10, batch_size=32):
        """å®Œæ•´çš„è®­ç»ƒè¿­ä»£"""
        print(f"\n=== è®­ç»ƒè¿­ä»£ {self.training_stats['iterations'] + 1} ===")
        
        # è‡ªæˆ‘å¯¹å¼ˆ
        games_generated = self.self_play_iteration(num_games=self_play_games)
        
        if games_generated > 0:
            # è®­ç»ƒç½‘ç»œ
            losses = self.train_network(epochs=training_epochs, batch_size=batch_size)
            
            self.training_stats['iterations'] += 1
            
            print(f"è®­ç»ƒè¿­ä»£å®Œæˆ")
            return True
        else:
            print(f"æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆæ¸¸æˆæ•°æ®ï¼Œè·³è¿‡è®­ç»ƒ")
            return False
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'model_state_dict': self.net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_stats': self.training_stats,
            'board_size': self.board_size,
            'num_channels': self.net.num_channels if hasattr(self.net, 'num_channels') else 64
        }
        
        torch.save(checkpoint, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(filepath):
            checkpoint = torch.load(filepath, map_location=self.device)
            
            self.net.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_stats = checkpoint.get('training_stats', self.training_stats)
            
            print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            print(f"è®­ç»ƒè¿­ä»£: {self.training_stats['iterations']}")
            print(f"è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆ: {self.training_stats['self_play_games']}")
            return True
        else:
            print(f"æ¨¡å‹æ–‡ä»¶ {filepath} ä¸å­˜åœ¨")
            return False
    
    def evaluate_model(self, opponent_model_path=None, num_games=10):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"\n=== æ¨¡å‹è¯„ä¼° ===")
        
        # åˆ›å»ºå½“å‰æ¨¡å‹çš„AI
        current_ai = GomokuAI(name="Current-Model", mcts_time=0.5)
        current_ai.net = self.net
        current_ai.mcts = MCTSWithNet(net=self.net, time_limit=0.5)
        
        # åˆ›å»ºå¯¹æ‰‹AI
        if opponent_model_path and os.path.exists(opponent_model_path):
            opponent_ai = GomokuAI(name="Opponent-Model", net_path=opponent_model_path, mcts_time=0.5)
        else:
            # ä½¿ç”¨çº¯MCTSä½œä¸ºå¯¹æ‰‹
            opponent_ai = GomokuAI(name="Pure-MCTS", mcts_time=0.5)
        
        # è¿›è¡Œå¯¹æˆ˜
        engine = GameEngine(board_size=self.board_size)
        wins = 0
        
        for game_num in range(num_games):
            if game_num % 2 == 0:
                winner, _ = engine.play_game(current_ai, opponent_ai, verbose=False)
                if winner == 1:  # å½“å‰æ¨¡å‹æ‰§é»‘è·èƒœ
                    wins += 1
            else:
                winner, _ = engine.play_game(opponent_ai, current_ai, verbose=False)
                if winner == -1:  # å½“å‰æ¨¡å‹æ‰§ç™½è·èƒœ
                    wins += 1
        
        win_rate = wins / num_games
        print(f"è¯„ä¼°ç»“æœ: {wins}/{num_games} èƒœ, èƒœç‡: {win_rate:.2%}")
        
        return win_rate

# æµ‹è¯•å‡½æ•°
def test_self_play():
    """æµ‹è¯•è‡ªæˆ‘å¯¹å¼ˆåŠŸèƒ½"""
    print("=== è‡ªæˆ‘å¯¹å¼ˆæµ‹è¯• ===")
    
    try:
        # åˆ›å»ºç®€å•çš„ç½‘ç»œ
        net = GomokuNet(board_size=9, num_channels=32)
        
        # åˆ›å»ºMCTSè®­ç»ƒå™¨
        mcts_trainer = MCTSTrainer(net=net, board_size=9, mcts_simulations=50)
        
        # è¿›è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆ
        states, policies, values = mcts_trainer.play_self_game(verbose=True)
        
        print(f"\nâœ“ è‡ªæˆ‘å¯¹å¼ˆæµ‹è¯•å®Œæˆ")
        print(f"ç”ŸæˆçŠ¶æ€æ•°: {len(states)}")
        print(f"ç­–ç•¥æ•°: {len(policies)}")
        print(f"ä»·å€¼æ•°: {len(values)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— è‡ªæˆ‘å¯¹å¼ˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_training():
    """æµ‹è¯•è®­ç»ƒåŠŸèƒ½"""
    print("\n=== è®­ç»ƒæµ‹è¯• ===")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = AlphaZeroTrainer(board_size=9, num_channels=32)
        
        # è¿›è¡Œä¸€è½®è®­ç»ƒè¿­ä»£
        success = trainer.training_iteration(
            self_play_games=2,
            training_epochs=2,
            batch_size=16
        )
        
        if success:
            print(f"\nâœ“ è®­ç»ƒæµ‹è¯•å®Œæˆ")
            print(f"è®­ç»ƒè¿­ä»£: {trainer.training_stats['iterations']}")
            print(f"æ•°æ®é›†å¤§å°: {len(trainer.dataset)}")
            return True
        else:
            print(f"âœ— è®­ç»ƒæµ‹è¯•å¤±è´¥: æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆæ•°æ®")
            return False
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("äº”å­æ£‹AIè®­ç»ƒæ¡†æ¶æµ‹è¯•")
    print("=" * 40)
    
    tests = [
        test_self_play,
        test_training
    ]
    
    passed = 0
    for test_func in tests:
        if test_func():
            passed += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{len(tests)} é€šè¿‡")
    
    if passed == len(tests):
        print("ğŸ‰ è®­ç»ƒæ¡†æ¶å®ç°æ­£å¸¸å·¥ä½œï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")