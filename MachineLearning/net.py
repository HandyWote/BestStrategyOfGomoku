#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº”å­æ£‹AIç¥ç»ç½‘ç»œæ¨¡å‹
å®ç°ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œï¼Œç”¨äºæŒ‡å¯¼MCTSæœç´¢
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
import json
import os

class ResidualBlock(nn.Module):
    """æ®‹å·®å—ï¼šæé«˜ç½‘ç»œæ·±åº¦å’Œç‰¹å¾æå–èƒ½åŠ›"""
    
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.dropout = nn.Dropout2d(0.1)  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.dropout(out)
        out = self.bn2(self.conv2(out))
        out += residual  # æ®‹å·®è¿æ¥
        out = F.relu(out)
        return out

class GomokuNet(nn.Module):
    """äº”å­æ£‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    
    ç»“åˆç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œï¼š
    - ç­–ç•¥ç½‘ç»œï¼šè¾“å‡ºæ¯ä¸ªä½ç½®çš„è½å­æ¦‚ç‡
    - ä»·å€¼ç½‘ç»œï¼šè¯„ä¼°å½“å‰å±€é¢çš„èƒœç‡
    
    æ”¹è¿›ç‰¹æ€§ï¼š
    - æ®‹å·®è¿æ¥ï¼šæé«˜ç½‘ç»œæ·±åº¦å’Œè®­ç»ƒç¨³å®šæ€§
    - Dropoutå±‚ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
    - æ”¹è¿›çš„æ‰¹å½’ä¸€åŒ–ï¼šæ›´å¥½çš„è®­ç»ƒæ”¶æ•›
    """
    
    def __init__(self, board_size=9, num_channels=64, num_residual_blocks=4):
        super(GomokuNet, self).__init__()
        self.board_size = board_size
        self.num_channels = num_channels
        self.num_residual_blocks = num_residual_blocks
        
        # è¾“å…¥å±‚ï¼šæ£‹ç›˜çŠ¶æ€ (batch_size, 3, board_size, board_size)
        # 3ä¸ªé€šé“ï¼šå½“å‰ç©å®¶æ£‹å­ã€å¯¹æ‰‹æ£‹å­ã€ç©ºä½ç½®
        
        # åˆå§‹å·ç§¯å±‚ï¼šå°†è¾“å…¥è½¬æ¢ä¸ºç‰¹å¾è¡¨ç¤º
        self.initial_conv = nn.Conv2d(3, num_channels, kernel_size=3, padding=1)
        self.initial_bn = nn.BatchNorm2d(num_channels)
        
        # æ®‹å·®å—ï¼šæ·±åº¦ç‰¹å¾æå–
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(num_channels) for _ in range(num_residual_blocks)
        ])
        
        # ç‰¹å¾æå–åçš„dropout
        self.feature_dropout = nn.Dropout2d(0.1)
        
        # ç­–ç•¥å¤´ï¼šè¾“å‡ºæ¯ä¸ªä½ç½®çš„è½å­æ¦‚ç‡
        self.policy_conv = nn.Conv2d(num_channels, 2, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(2)
        self.policy_fc = nn.Linear(2 * board_size * board_size, board_size * board_size)
        
        # ä»·å€¼å¤´ï¼šè¯„ä¼°å±€é¢ä»·å€¼
        self.value_conv = nn.Conv2d(num_channels, 1, kernel_size=1)
        self.value_bn = nn.BatchNorm2d(1)
        self.value_fc1 = nn.Linear(board_size * board_size, 256)
        self.value_fc2 = nn.Linear(256, 1)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        
        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, 3, board_size, board_size)
            
        Returns:
            policy: ç­–ç•¥æ¦‚ç‡ (batch_size, board_size * board_size)
            value: å±€é¢ä»·å€¼ (batch_size, 1)
        """
        # åˆå§‹ç‰¹å¾æå–
        x = F.relu(self.initial_bn(self.initial_conv(x)))
        
        # é€šè¿‡æ®‹å·®å—è¿›è¡Œæ·±åº¦ç‰¹å¾æå–
        for residual_block in self.residual_blocks:
            x = residual_block(x)
        
        # ç‰¹å¾dropout
        x = self.feature_dropout(x)
        
        # ç­–ç•¥å¤´
        policy = F.relu(self.policy_bn(self.policy_conv(x)))
        policy = policy.view(policy.size(0), -1)  # å±•å¹³
        policy = F.log_softmax(self.policy_fc(policy), dim=1)
        
        # ä»·å€¼å¤´
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(value.size(0), -1)  # å±•å¹³
        value = F.relu(self.value_fc1(value))
        value = torch.tanh(self.value_fc2(value))  # è¾“å‡ºèŒƒå›´ [-1, 1]
        
        return policy, value
    
    def predict(self, board_state):
        """é¢„æµ‹å•ä¸ªæ£‹ç›˜çŠ¶æ€
        
        Args:
            board_state: GomokuBoardå®ä¾‹
            
        Returns:
            policy_probs: ç­–ç•¥æ¦‚ç‡æ•°ç»„ (board_size * board_size,)
            value: å±€é¢ä»·å€¼ (æ ‡é‡)
        """
        self.eval()
        with torch.no_grad():
            # è½¬æ¢æ£‹ç›˜çŠ¶æ€ä¸ºç½‘ç»œè¾“å…¥
            input_tensor = self._board_to_tensor(board_state)
            input_tensor = input_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            # å‰å‘ä¼ æ’­
            policy_log_probs, value = self.forward(input_tensor)
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            policy_probs = torch.exp(policy_log_probs).squeeze(0).numpy()
            value = value.squeeze(0).item()
            
            return policy_probs, value
    
    def _board_to_tensor(self, board_state):
        """å°†æ£‹ç›˜çŠ¶æ€è½¬æ¢ä¸ºç½‘ç»œè¾“å…¥å¼ é‡
        
        Args:
            board_state: GomokuBoardå®ä¾‹
            
        Returns:
            tensor: (3, board_size, board_size) å¼ é‡
        """
        board = board_state.board
        current_player = board_state.current_player
        
        # åˆ›å»º3ä¸ªé€šé“
        channels = np.zeros((3, self.board_size, self.board_size), dtype=np.float32)
        
        # é€šé“0ï¼šå½“å‰ç©å®¶çš„æ£‹å­
        channels[0] = (board == current_player).astype(np.float32)
        
        # é€šé“1ï¼šå¯¹æ‰‹çš„æ£‹å­
        channels[1] = (board == -current_player).astype(np.float32)
        
        # é€šé“2ï¼šç©ºä½ç½®
        channels[2] = (board == 0).astype(np.float32)
        
        return torch.FloatTensor(channels)

class GomokuDataset(Dataset):
    """äº”å­æ£‹è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_file=None):
        self.data = []
        if data_file and os.path.exists(data_file):
            self.load_data(data_file)
    
    def add_sample(self, board_state, policy_target, value_target):
        """æ·»åŠ è®­ç»ƒæ ·æœ¬
        
        Args:
            board_state: GomokuBoardå®ä¾‹
            policy_target: ç­–ç•¥ç›®æ ‡ (board_size * board_size,)
            value_target: ä»·å€¼ç›®æ ‡ (æ ‡é‡)
        """
        self.data.append({
            'board': board_state.board.copy(),
            'current_player': board_state.current_player,
            'policy': policy_target.copy() if isinstance(policy_target, np.ndarray) else policy_target,
            'value': float(value_target)
        })
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # é‡å»ºæ£‹ç›˜çŠ¶æ€
        from board import GomokuBoard
        board_state = GomokuBoard(size=len(sample['board']))
        board_state.board = np.array(sample['board'])
        board_state.current_player = sample['current_player']
        
        # è½¬æ¢ä¸ºå¼ é‡
        net = GomokuNet(board_size=len(sample['board']))
        input_tensor = net._board_to_tensor(board_state)
        
        policy_tensor = torch.FloatTensor(sample['policy'])
        value_tensor = torch.FloatTensor([sample['value']])
        
        return input_tensor, policy_tensor, value_tensor
    
    def save_data(self, filename):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        with open(filename, 'w') as f:
            json.dump(self.data, f)
    
    def load_data(self, filename):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
        with open(filename, 'r') as f:
            self.data = json.load(f)

class GomokuTrainer:
    """äº”å­æ£‹ç½‘ç»œè®­ç»ƒå™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    
    æ”¹è¿›ç‰¹æ€§ï¼š
    - å­¦ä¹ ç‡è°ƒåº¦ï¼šè‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
    - æ—©åœæœºåˆ¶ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
    - æ¢¯åº¦è£å‰ªï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
    - è¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡ï¼šç›‘æ§è®­ç»ƒè¿›åº¦
    """
    
    def __init__(self, net, learning_rate=0.001, device=None, patience=10):
        self.net = net
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.net.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šå½“éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶å‡å°‘å­¦ä¹ ç‡
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
        
        # æ—©åœæœºåˆ¶
        self.patience = patience
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.early_stop = False
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'total_loss': [],
            'policy_loss': [],
            'value_loss': [],
            'learning_rate': []
        }
        
        # æŸå¤±å‡½æ•°
        self.policy_loss_fn = nn.KLDivLoss(reduction='batchmean')
        self.value_loss_fn = nn.MSELoss()
        
        print(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def train_step(self, dataloader, policy_weight=1.0, value_weight=1.0):
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            policy_weight: ç­–ç•¥æŸå¤±æƒé‡
            value_weight: ä»·å€¼æŸå¤±æƒé‡
            
        Returns:
            dict: åŒ…å«è¯¦ç»†è®­ç»ƒæŒ‡æ ‡çš„å­—å…¸
        """
        self.net.train()
        total_loss = 0.0
        total_policy_loss = 0.0
        total_value_loss = 0.0
        num_batches = 0
        
        for batch_inputs, batch_policy_targets, batch_value_targets in dataloader:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch_inputs = batch_inputs.to(self.device)
            batch_policy_targets = batch_policy_targets.to(self.device)
            batch_value_targets = batch_value_targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            policy_outputs, value_outputs = self.net(batch_inputs)
            
            # è®¡ç®—æŸå¤±
            policy_loss = self.policy_loss_fn(policy_outputs, batch_policy_targets)
            value_loss = self.value_loss_fn(value_outputs, batch_value_targets)
            
            # æ€»æŸå¤±
            loss = policy_weight * policy_loss + value_weight * value_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            num_batches += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_total_loss = total_loss / num_batches
        avg_policy_loss = total_policy_loss / num_batches
        avg_value_loss = total_value_loss / num_batches
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_history['total_loss'].append(avg_total_loss)
        self.train_history['policy_loss'].append(avg_policy_loss)
        self.train_history['value_loss'].append(avg_value_loss)
        self.train_history['learning_rate'].append(current_lr)
        
        return {
            'total_loss': avg_total_loss,
            'policy_loss': avg_policy_loss,
            'value_loss': avg_value_loss,
            'learning_rate': current_lr,
            'num_batches': num_batches
        }
    
    def validate(self, val_dataloader):
        """éªŒè¯æ¨¡å‹æ€§èƒ½
        
        Args:
            val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            dict: éªŒè¯æŒ‡æ ‡
        """
        self.net.eval()
        total_loss = 0.0
        total_policy_loss = 0.0
        total_value_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch_inputs, batch_policy_targets, batch_value_targets in val_dataloader:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch_inputs = batch_inputs.to(self.device)
                batch_policy_targets = batch_policy_targets.to(self.device)
                batch_value_targets = batch_value_targets.to(self.device)
                
                # å‰å‘ä¼ æ’­
                policy_outputs, value_outputs = self.net(batch_inputs)
                
                # è®¡ç®—æŸå¤±
                policy_loss = self.policy_loss_fn(policy_outputs, batch_policy_targets)
                value_loss = self.value_loss_fn(value_outputs, batch_value_targets)
                loss = policy_loss + value_loss
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                num_batches += 1
        
        avg_val_loss = total_loss / num_batches
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler.step(avg_val_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < self.best_loss:
            self.best_loss = avg_val_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                self.early_stop = True
        
        return {
            'val_total_loss': avg_val_loss,
            'val_policy_loss': total_policy_loss / num_batches,
            'val_value_loss': total_value_loss / num_batches,
            'early_stop': self.early_stop,
            'patience_counter': self.patience_counter
        }
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        torch.save({
            'model_state_dict': self.net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'board_size': self.net.board_size,
            'num_channels': self.net.num_channels,
            'num_residual_blocks': self.net.num_residual_blocks,
            'train_history': self.train_history,
            'best_loss': self.best_loss,
            'patience_counter': self.patience_counter
        }, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.net.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # åŠ è½½è®­ç»ƒå†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'train_history' in checkpoint:
            self.train_history = checkpoint['train_history']
        
        # åŠ è½½æ—©åœç›¸å…³çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'best_loss' in checkpoint:
            self.best_loss = checkpoint['best_loss']
        if 'patience_counter' in checkpoint:
            self.patience_counter = checkpoint['patience_counter']
        
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")
        print(f"å½“å‰è€å¿ƒè®¡æ•°: {self.patience_counter}/{self.patience}")

# æµ‹è¯•å‡½æ•°
def test_network():
    """æµ‹è¯•ç¥ç»ç½‘ç»œåŸºæœ¬åŠŸèƒ½"""
    print("=== ç¥ç»ç½‘ç»œæµ‹è¯• ===")
    
    try:
        # åˆ›å»ºç½‘ç»œ
        net = GomokuNet(board_size=9, num_channels=32)
        print(f"âœ“ ç½‘ç»œåˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in net.parameters())}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 4
        input_tensor = torch.randn(batch_size, 3, 9, 9)
        policy, value = net(input_tensor)
        
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  ç­–ç•¥è¾“å‡ºå½¢çŠ¶: {policy.shape}")
        print(f"  ä»·å€¼è¾“å‡ºå½¢çŠ¶: {value.shape}")
        
        # æµ‹è¯•å•ä¸ªé¢„æµ‹
        from board import GomokuBoard
        board = GomokuBoard(size=9)
        board.make_move(4, 4)
        
        policy_probs, value_pred = net.predict(board)
        print(f"âœ“ å•ä¸ªé¢„æµ‹æˆåŠŸ")
        print(f"  ç­–ç•¥æ¦‚ç‡å’Œ: {np.sum(policy_probs):.3f}")
        print(f"  ä»·å€¼é¢„æµ‹: {value_pred:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_training():
    """æµ‹è¯•è®­ç»ƒåŠŸèƒ½"""
    print("\n=== è®­ç»ƒåŠŸèƒ½æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºç½‘ç»œå’Œè®­ç»ƒå™¨
        net = GomokuNet(board_size=9, num_channels=16)  # å°ç½‘ç»œç”¨äºæµ‹è¯•
        trainer = GomokuTrainer(net, learning_rate=0.01)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        dataset = GomokuDataset()
        
        from board import GomokuBoard
        for i in range(10):  # åˆ›å»º10ä¸ªæ ·æœ¬
            board = GomokuBoard(size=9)
            # éšæœºä¸‹å‡ æ­¥æ£‹
            for _ in range(np.random.randint(1, 5)):
                valid_moves = board.get_valid_moves()
                if valid_moves:
                    move = np.random.choice(len(valid_moves))
                    board.make_move(valid_moves[move][0], valid_moves[move][1])
            
            # åˆ›å»ºéšæœºç›®æ ‡
            policy_target = np.random.random(81)
            policy_target = policy_target / np.sum(policy_target)  # å½’ä¸€åŒ–
            value_target = np.random.uniform(-1, 1)
            
            dataset.add_sample(board, policy_target, value_target)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
        
        # æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        total_loss, policy_loss, value_loss = trainer.train_step(dataloader)
        
        print(f"âœ“ è®­ç»ƒæ­¥éª¤æˆåŠŸ")
        print(f"  æ€»æŸå¤±: {total_loss:.4f}")
        print(f"  ç­–ç•¥æŸå¤±: {policy_loss:.4f}")
        print(f"  ä»·å€¼æŸå¤±: {value_loss:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("äº”å­æ£‹ç¥ç»ç½‘ç»œæµ‹è¯•")
    print("=" * 40)
    
    tests = [
        test_network,
        test_training
    ]
    
    passed = 0
    for test_func in tests:
        if test_func():
            passed += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{len(tests)} é€šè¿‡")
    
    if passed == len(tests):
        print("ğŸ‰ ç¥ç»ç½‘ç»œå®ç°æ­£å¸¸å·¥ä½œï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")